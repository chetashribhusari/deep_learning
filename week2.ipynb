{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMygZFhOx+Mti//rsitd/ek",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chetashribhusari/deep_learning/blob/master/week2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibX-gsPnPbZt",
        "colab_type": "code",
        "outputId": "067958bb-ab21-4ecf-ed72-00df01abacd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# MNIST digits classification with TensorFlow\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"We're using TF\", tf.__version__)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We're using TF 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LSs4jUhPrx9",
        "colab_type": "code",
        "outputId": "67ee840b-03e4-47c9-cb22-e7d0e2b306d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "\n",
        "import sys\n",
        "sys.path.append(\"../..\")\n",
        "import matplotlib_utils\n",
        "from importlib import reload\n",
        "reload(matplotlib_utils)\n",
        "import grading_utils\n",
        "reload(grading_utils)\n",
        "import keras_utils\n",
        "from keras_utils import reset_tf_session"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-60ad984f9e68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib_utils\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib_utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UODHBZGKW_Ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA5AbCy-Ps4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import grading\n",
        "grader = grading.Grader(assignment_key=\"XtD7ho3TEeiHQBLWejjYAA\", \n",
        "                        all_parts=[\"9XaAS\", \"vmogZ\", \"RMv95\", \"i8bgs\", \"rE763\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMxmGHCUPszo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# token expires every 30 min\n",
        "COURSERA_TOKEN = \"\"\n",
        "COURSERA_EMAIL = \"chetashri.bhusari@vpt.edu.in\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rERku-oWPsxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import preprocessed_mnist\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = preprocessed_mnist.load_dataset_from_file()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-hOlwEHPsun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X contains rgb values divided by 255\n",
        "print(\"X_train [shape %s] sample patch:\\n\" % (str(X_train.shape)), X_train[1, 15:20, 5:10])\n",
        "print(\"A closeup of a sample patch:\")\n",
        "plt.imshow(X_train[1, 15:20, 5:10], cmap=\"Greys\")\n",
        "plt.show()\n",
        "print(\"And the whole sample:\")\n",
        "plt.imshow(X_train[1], cmap=\"Greys\")\n",
        "plt.show()\n",
        "print(\"y_train [shape %s] 10 samples:\\n\" % (str(y_train.shape)), y_train[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Myg77oKPsrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
        "print(X_train_flat.shape)\n",
        "\n",
        "X_val_flat = X_val.reshape((X_val.shape[0], -1))\n",
        "print(X_val_flat.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTknIfM5PspZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "\n",
        "y_train_oh = keras.utils.to_categorical(y_train, 10)\n",
        "y_val_oh = keras.utils.to_categorical(y_val, 10)\n",
        "\n",
        "print(y_train_oh.shape)\n",
        "print(y_train_oh[:3], y_train[:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnB1bLOCPsnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this again if you remake your graph\n",
        "s = reset_tf_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PY5UgsTPsk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model parameters: W and b \n",
        "W = tf.get_variable(\"W\", shape=(784, 10), dtype=tf.float32, trainable=True) ### tf.get_variable(...) with shape[0] = 784\n",
        "b = tf.get_variable(\"b\", shape=(10,), dtype=tf.float32) ### tf.get_variable(...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrNOM4eAPshn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Placeholders for the input data\n",
        "input_X = tf.placeholder(tf.float32, shape=(None, 784)) ### tf.placeholder(...) for flat X with shape[0] = None for any batch size\n",
        "input_y = tf.placeholder(tf.float32, shape=(None, 10))###  tf.placeholder(...) for one-hot encoded true labels\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxzXbmOpPse2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute predictions\n",
        "logits = input_X @ W + b ### logits for input_X, resulting shape should be [input_X.shape[0], 10]\n",
        "probas = tf.nn.softmax(logits) ### apply tf.nn.softmax to logits\n",
        "classes = tf.argmax(probas,1) ### apply tf.argmax to find a class index with highest probability\n",
        "\n",
        "# Loss should be a scalar number: average loss over all the objects with tf.reduce_mean().\n",
        "# Use tf.nn.softmax_cross_entropy_with_logits on top of one-hot encoded input_y and logits.\n",
        "# It is identical to calculating cross-entropy on top of probas, but is more numerically friendly (read the docs).\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=input_y, logits=logits))### YOUR CODE HERE ### cross-entropy loss\n",
        "\n",
        "# Use a default tf.train.AdamOptimizer to get an SGD step\n",
        "step = tf.train.AdamOptimizer().minimize(loss) ### optimizer step that minimizes the loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6mtY309PscQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s.run(tf.global_variables_initializer())\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 40\n",
        "\n",
        "# for logging the progress right here in Jupyter (for those who don't have TensorBoard)\n",
        "simpleTrainingCurves = matplotlib_utils.SimpleTrainingCurves(\"cross-entropy\", \"accuracy\")\n",
        "\n",
        "for epoch in range(EPOCHS):  # we finish an epoch when we've looked at all training samples\n",
        "    \n",
        "    batch_losses = []\n",
        "    for batch_start in range(0, X_train_flat.shape[0], BATCH_SIZE):  # data is already shuffled\n",
        "        _, batch_loss = s.run([step, loss], {input_X: X_train_flat[batch_start:batch_start+BATCH_SIZE], \n",
        "                                             input_y: y_train_oh[batch_start:batch_start+BATCH_SIZE]})\n",
        "        # collect batch losses, this is almost free as we need a forward pass for backprop anyway\n",
        "        batch_losses.append(batch_loss)\n",
        "\n",
        "    train_loss = np.mean(batch_losses)\n",
        "    val_loss = s.run(loss, {input_X: X_val_flat, input_y: y_val_oh})  # this part is usually small\n",
        "    train_accuracy = accuracy_score(y_train, s.run(classes, {input_X: X_train_flat}))  # this is slow and usually skipped\n",
        "    valid_accuracy = accuracy_score(y_val, s.run(classes, {input_X: X_val_flat}))  \n",
        "    simpleTrainingCurves.add(train_loss, val_loss, train_accuracy, valid_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njh_Zj14PsaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## GRADED PART, DO NOT CHANGE!\n",
        "# Testing shapes \n",
        "grader.set_answer(\"9XaAS\", grading_utils.get_tensors_shapes_string([W, b, input_X, input_y, logits, probas, classes]))\n",
        "# Validation loss\n",
        "grader.set_answer(\"vmogZ\", s.run(loss, {input_X: X_val_flat, input_y: y_val_oh}))\n",
        "# Validation accuracy\n",
        "grader.set_answer(\"RMv95\", accuracy_score(y_val, s.run(classes, {input_X: X_val_flat})))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s5kXhIWPsXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# you can make submission with answers so far to check yourself at this stage\n",
        "grader.submit(\"Chetashri.bhusari@vpt.edu.in\", \"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk7rQtU_PsUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write the code here to get a new `step` operation and then run the cell with training loop above.\n",
        "# name your variables in the same way (e.g. logits, probas, classes, etc) for safety.\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "hidden1 = tf.layers.dense(input_X, 256, activation=tf.nn.sigmoid)\n",
        "hidden2 = tf.layers.dense(hidden1, 256, activation=tf.nn.sigmoid)\n",
        "logits = tf.layers.dense(hidden2, 10)\n",
        "\n",
        "probas = tf.nn.softmax(logits)\n",
        "classes = tf.argmax(probas,1)\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=input_y))\n",
        "\n",
        "step = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "s.run(tf.global_variables_initializer())\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 40\n",
        "\n",
        "# for logging the progress right here in Jupyter (for those who don't have TensorBoard)\n",
        "simpleTrainingCurves = matplotlib_utils.SimpleTrainingCurves(\"cross-entropy\", \"accuracy\")\n",
        "\n",
        "for epoch in range(EPOCHS):  # we finish an epoch when we've looked at all training samples\n",
        "    \n",
        "    batch_losses = []\n",
        "    for batch_start in range(0, X_train_flat.shape[0], BATCH_SIZE):  # data is already shuffled\n",
        "        _, batch_loss = s.run([step, loss], {input_X: X_train_flat[batch_start:batch_start+BATCH_SIZE], \n",
        "                                             input_y: y_train_oh[batch_start:batch_start+BATCH_SIZE]})\n",
        "        # collect batch losses, this is almost free as we need a forward pass for backprop anyway\n",
        "        batch_losses.append(batch_loss)\n",
        "\n",
        "    train_loss = np.mean(batch_losses)\n",
        "    val_loss = s.run(loss, {input_X: X_val_flat, input_y: y_val_oh})  # this part is usually small\n",
        "    train_accuracy = accuracy_score(y_train, s.run(classes, {input_X: X_train_flat}))  # this is slow and usually skipped\n",
        "    valid_accuracy = accuracy_score(y_val, s.run(classes, {input_X: X_val_flat}))  \n",
        "    simpleTrainingCurves.add(train_loss, val_loss, train_accuracy, valid_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYx4ufOQPsRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## GRADED PART, DO NOT CHANGE!\n",
        "# Validation loss for MLP\n",
        "grader.set_answer(\"i8bgs\", s.run(loss, {input_X: X_val_flat, input_y: y_val_oh}))\n",
        "# Validation accuracy for MLP\n",
        "grader.set_answer(\"rE763\", accuracy_score(y_val, s.run(classes, {input_X: X_val_flat})))\n",
        "# you can make submission with answers so far to check yourself at this stage\n",
        "grader.submit(\"chetashri.bhusari@vpt.edu.in\", \"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACEHR-62PsLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTmKM9hGPsJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frmr246iPsHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlyUoIQjPsE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGhGD_zHPsBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8oU_ZQhPr86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}